"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ Python –∫–æ–¥–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç.
"""

import ast
import subprocess
import json
import os
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, asdict
import pandas as pd
from radon.complexity import cc_visit
from radon.metrics import h_visit, mi_visit


@dataclass
class CodeMetrics:
    """–•—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""

    filename: str
    lines_of_code: int
    num_functions: int
    num_classes: int
    avg_complexity: float
    max_complexity: int
    maintainability_index: float
    pep8_errors: int
    pep8_warnings: int
    docstring_coverage: float  # % —Ñ—É–Ω–∫—Ü–∏–π/–∫–ª–∞—Å—Å–æ–≤ —Å docstring
    comment_density: float  # % —Å—Ç—Ä–æ–∫ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
    score: float  # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ 0-100


class CodeAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Python –∫–æ–¥–∞."""

    def __init__(self):
        self.metrics_weights = {
            "pep8": 0.25,
            "complexity": 0.25,
            "maintainability": 0.20,
            "docstrings": 0.15,
            "comments": 0.15,
        }

    def analyze_file(self, filepath: str) -> CodeMetrics:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω Python —Ñ–∞–π–ª."""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                code = f.read()
        except UnicodeDecodeError:
            try:
                with open(filepath, "r", encoding="cp1251") as f:
                    code = f.read()
            except Exception as e:
                return self._create_error_metrics(filepath, f"Encoding Error: {str(e)}")
        except FileNotFoundError:
            return self._create_error_metrics(filepath, "File Not Found")
        except Exception as e:
            return self._create_error_metrics(filepath, f"Read Error: {str(e)}")

        try:
            tree = ast.parse(code)
        except (SyntaxError, ValueError) as e:
            return self._create_error_metrics(filepath, f"Syntax Error: {str(e)}")

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ AST
        loc = len(code.split("\n"))
        num_funcs = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])
        num_classes = len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])

        # –¶–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ radon
        try:
            complexity_results = cc_visit(code)
            complexities = [c.complexity for c in complexity_results]
            avg_complexity = (
                sum(complexities) / len(complexities) if complexities else 0
            )
            max_complexity = max(complexities) if complexities else 0
        except Exception:
            avg_complexity = 0
            max_complexity = 0

        # –ò–Ω–¥–µ–∫—Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏
        try:
            mi_score = mi_visit(code, multi=True)
            if isinstance(mi_score, (int, float)):
                mi_score = float(mi_score)
            else:
                mi_score = 0.0
        except Exception:
            mi_score = 0.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ PEP8
        pep8_errors, pep8_warnings = self._run_pep8_check(filepath)

        # Docstring coverage
        docstring_coverage = self._calculate_docstring_coverage(
            tree, num_funcs, num_classes
        )

        # Comment density
        comment_density = self._calculate_comment_density(code)

        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        score = self._calculate_score(
            {
                "pep8_errors": pep8_errors,
                "pep8_warnings": pep8_warnings,
                "avg_complexity": avg_complexity,
                "max_complexity": max_complexity,
                "maintainability": mi_score,
                "docstring_coverage": docstring_coverage,
                "comment_density": comment_density,
            }
        )

        return CodeMetrics(
            filename=os.path.basename(filepath),
            lines_of_code=loc,
            num_functions=num_funcs,
            num_classes=num_classes,
            avg_complexity=round(avg_complexity, 2),
            max_complexity=max_complexity,
            maintainability_index=round(mi_score, 2),
            pep8_errors=pep8_errors,
            pep8_warnings=pep8_warnings,
            docstring_coverage=round(docstring_coverage, 2),
            comment_density=round(comment_density, 2),
            score=round(score, 2),
        )

    def analyze_directory(self, directory: str) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ Python —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        results = []

        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(".py"):
                    filepath = os.path.join(root, file)
                    metrics = self.analyze_file(filepath)
                    results.append(asdict(metrics))

        return results

    def _run_pep8_check(self, filepath: str) -> Tuple[int, int]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É PEP8 —á–µ—Ä–µ–∑ flake8."""
        try:
            errors = 0
            warnings = 0

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ (E, F)
            result_errors = subprocess.run(
                ["flake8", "--select=E,F", "--count", "--max-line-length=88", filepath],
                capture_output=True,
                text=True,
                timeout=30,
                check=False,
            )
            if result_errors.stdout.strip().isdigit():
                errors = int(result_errors.stdout.strip())

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (W)
            result_warnings = subprocess.run(
                ["flake8", "--select=W", "--count", "--max-line-length=88", filepath],
                capture_output=True,
                text=True,
                timeout=30,
                check=False,
            )
            if result_warnings.stdout.strip().isdigit():
                warnings = int(result_warnings.stdout.strip())

            return errors, warnings

        except subprocess.TimeoutExpired:
            print(f"Timeout –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {filepath}")
            return 0, 0
        except FileNotFoundError:
            print("flake8 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install flake8")
            return 0, 0
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ PEP8: {str(e)}")
            return 0, 0

    def _calculate_docstring_coverage(
        self, tree: ast.AST, num_funcs: int, num_classes: int
    ) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ docstring."""
        total_entities = num_funcs + num_classes
        if total_entities == 0:
            return 100.0

        entities_with_doc = 0

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                docstring = ast.get_docstring(node)
                if docstring and len(docstring.strip()) > 0:
                    entities_with_doc += 1

        return (entities_with_doc / total_entities) * 100

    def _calculate_comment_density(self, code: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""
        lines = code.split("\n")
        total_lines = len(lines)
        if total_lines == 0:
            return 0.0

        comment_lines = sum(1 for line in lines if line.strip().startswith("#"))
        return (comment_lines / total_lines) * 100

    def _calculate_score(self, metrics: Dict[str, float]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É 0-100 –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫."""
        score = 100

        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –æ—à–∏–±–∫–∏ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
        score -= min(metrics["pep8_errors"] * 2, 30)  # –ú–∞–∫—Å 30 –±–∞–ª–ª–æ–≤ —à—Ç—Ä–∞—Ñ–∞
        score -= min(metrics["pep8_warnings"] * 0.5, 10)  # –ú–∞–∫—Å 10 –±–∞–ª–ª–æ–≤

        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        if metrics["max_complexity"] > 10:
            score -= min((metrics["max_complexity"] - 10) * 2, 20)
        if metrics["avg_complexity"] > 5:
            score -= min((metrics["avg_complexity"] - 5) * 1, 10)

        # –ë–æ–Ω—É—Å/—à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å
        if metrics["maintainability"] > 80:
            score += 5
        elif metrics["maintainability"] < 40:
            score -= 10

        # –ë–æ–Ω—É—Å –∑–∞ docstring (–º–∞–∫—Å 15 –±–∞–ª–ª–æ–≤)
        score += min(metrics["docstring_coverage"] * 0.15, 15)

        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ)
        if 5 <= metrics["comment_density"] <= 20:
            score += 5
        elif metrics["comment_density"] < 2:
            score -= 5

        return max(0, min(100, score))

    def _create_error_metrics(self, filename: str, error_msg: str) -> CodeMetrics:
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ —Å –æ—à–∏–±–∫–æ–π."""
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {filename}: {error_msg}")
        return CodeMetrics(
            filename=os.path.basename(filename),
            lines_of_code=0,
            num_functions=0,
            num_classes=0,
            avg_complexity=0,
            max_complexity=0,
            maintainability_index=0,
            pep8_errors=100,
            pep8_warnings=0,
            docstring_coverage=0,
            comment_density=0,
            score=0,
        )

    def generate_report(self, results: List[Dict[str, Any]], output_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –∏ CSV."""
        if not results:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞")
            return ""

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        json_path = output_path + ".json"
        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "timestamp": pd.Timestamp.now().isoformat(),
                        "total_files": len(results),
                        "average_score": round(
                            sum(r["score"] for r in results) / len(results), 2
                        ),
                        "files": results,
                    },
                    f,
                    indent=2,
                    ensure_ascii=False,
                )
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ JSON: {str(e)}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        csv_path = output_path + ".csv"
        try:
            df = pd.DataFrame(results)
            df.to_csv(csv_path, index=False, encoding="utf-8")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ CSV: {str(e)}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        try:
            df = pd.DataFrame(results)
            summary = {
                "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞": {
                    "–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤": len(results),
                    "–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞": round(df["score"].mean(), 2),
                    "–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç": round(df["score"].max(), 2),
                    "–•—É–¥—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç": round(df["score"].min(), 2),
                    "–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å": round(df["avg_complexity"].mean(), 2),
                    "–°—Ä–µ–¥–Ω–∏–π –∏–Ω–¥–µ–∫—Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏": round(
                        df["maintainability_index"].mean(), 2
                    ),
                    "–°—Ä–µ–¥–Ω–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ docstring": round(
                        df["docstring_coverage"].mean(), 2
                    ),
                    "–°—Ä–µ–¥–Ω—è—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤": round(
                        df["comment_density"].mean(), 2
                    ),
                }
            }

            summary_path = output_path + "_summary.txt"
            with open(summary_path, "w", encoding="utf-8") as f:
                for section, data in summary.items():
                    f.write(f"{section}\n")
                    f.write("=" * 40 + "\n")
                    for key, value in data.items():
                        f.write(f"{key}: {value}\n")
                    f.write("\n")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: {str(e)}")

        return json_path


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    analyzer = CodeAnalyzer()
